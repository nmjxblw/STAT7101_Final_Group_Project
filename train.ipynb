{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1239a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38393096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"freq_features_data.csv\")\n",
    "\n",
    "X = df.drop('inv_key', axis=1)\n",
    "y = df['inv_key']\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0ac619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 26])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {chr(ord('a') + i): i for i in range(26)}\n",
    "idx2char = {i: chr(ord('a') + i) for i in range(26)}\n",
    "letters = [i for i in 'abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "def preprocess_X(X_train, X_val):\n",
    "    \"\"\"标准化输入特征（训练集拟合，验证集转换）\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    # 转换为PyTorch张量（float32）\n",
    "    return torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "\n",
    "def preprocess_y(Y):\n",
    "    Y = [[i for i in list(y) if i in letters] for y in Y]\n",
    "    y_idx = np.array([[char2idx[c] for c in row] for row in Y])  # (样本数, 26)\n",
    "    return torch.tensor(y_idx, dtype=torch.long)  # 类索引（long类型）\n",
    "\n",
    "preprocess_y(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9e7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_DIM = 702  # 输入特征维度\n",
    "NUM_TASKS = 26  # 26个分类任务\n",
    "NUM_CLASSES_PER_TASK = 26  # 每个任务26个类别（a-z）\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 80\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# ---------------------- 3. 自定义数据集类 ----------------------\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # (样本数, 702) 张量\n",
    "        self.y = y  # (样本数, 26) 张量（每个元素是0-25的类索引）\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]  # 返回单个样本（特征+26个任务的标签）\n",
    "\n",
    "# ---------------------- 4. MLP模型定义（共享特征+多任务输出） ----------------------\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskMLP, self).__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(INPUT_DIM, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(512, HIDDEN_DIM),\n",
    "            nn.BatchNorm1d(HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.BatchNorm1d(HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.BatchNorm1d(HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Linear(HIDDEN_DIM, NUM_CLASSES_PER_TASK) for _ in range(NUM_TASKS)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播：输入→共享特征→26个任务输出\"\"\"\n",
    "        shared_features = self.shared_layers(x)\n",
    "        outputs = [head(shared_features) for head in self.task_heads]\n",
    "        return outputs\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # 记录每个任务的总正确数和总样本数\n",
    "    task_correct = [0] * NUM_TASKS\n",
    "    task_total = [0] * NUM_TASKS\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)  # (batch_size, 26)\n",
    "        \n",
    "        # 前向传播：获取26个任务的输出（每个输出是(batch_size, 26)）\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # 计算每个任务的损失，求和作为总损失\n",
    "        loss = 0.0\n",
    "        for i in range(NUM_TASKS):\n",
    "            task_output = outputs[i]  # (batch_size, 26)\n",
    "            task_y = y_batch[:, i]    # (batch_size,) 第i个任务的标签\n",
    "            loss += criterion(task_output, task_y)\n",
    "        \n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累计损失\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # 计算每个任务的准确率\n",
    "        for i in range(NUM_TASKS):\n",
    "            task_output = outputs[i]\n",
    "            task_y = y_batch[:, i]\n",
    "            _, predicted = torch.max(task_output, 1)  # 预测类别索引\n",
    "            task_correct[i] += (predicted == task_y).sum().item()\n",
    "            task_total[i] += task_y.size(0)\n",
    "    \n",
    "    # 计算平均损失和每个任务的准确率\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    task_accs = [task_correct[i] / task_total[i] for i in range(NUM_TASKS)]\n",
    "    return avg_loss, task_accs\n",
    "\n",
    "def val_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    task_correct = [0] * NUM_TASKS\n",
    "    task_total = [0] * NUM_TASKS\n",
    "    \n",
    "    with torch.no_grad():  # 验证时不计算梯度\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = 0.0\n",
    "            for i in range(NUM_TASKS):\n",
    "                task_output = outputs[i]\n",
    "                task_y = y_batch[:, i]\n",
    "                loss += criterion(task_output, task_y)\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            # 计算准确率\n",
    "            for i in range(NUM_TASKS):\n",
    "                task_output = outputs[i]\n",
    "                task_y = y_batch[:, i]\n",
    "                _, predicted = torch.max(task_output, 1)\n",
    "                task_correct[i] += (predicted == task_y).sum().item()\n",
    "                task_total[i] += task_y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    task_accs = [task_correct[i] / task_total[i] for i in range(NUM_TASKS)]\n",
    "    return avg_loss, task_accs\n",
    "\n",
    "def predict(model, X_val, device):\n",
    "    model.eval()\n",
    "    X_val = X_val.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)  # 26个任务的输出，每个(batch_size, 26)\n",
    "    \n",
    "    # 概率→类别索引→字符\n",
    "    y_pred = []\n",
    "    for sample_idx in range(len(X_val)):\n",
    "        sample_chars = []\n",
    "        for task_idx in range(NUM_TASKS):\n",
    "            task_output = outputs[task_idx][sample_idx]  # (26,)\n",
    "            pred_idx = torch.argmax(task_output).item()  # 预测索引\n",
    "            sample_chars.append(idx2char[pred_idx])      # 转换为字符\n",
    "        y_pred.append(sample_chars)\n",
    "    \n",
    "    return y_pred  # 形状：(样本数, 26)，与原始y格式一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c56ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80]\n",
      "Train Loss: 83.5253 | Avg Train Acc: 0.0709\n",
      "Val Loss: 79.9465 | Avg Val Acc: 0.0973\n",
      "Task Accs (0-25): ['0.0993', '0.0940', '0.1060', '0.0917', '0.1007']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 1 | Val Acc: 0.0973)\n",
      "Best model saved (Epoch: 2 | Val Acc: 0.1222)\n",
      "Best model saved (Epoch: 3 | Val Acc: 0.1380)\n",
      "Best model saved (Epoch: 4 | Val Acc: 0.1564)\n",
      "Epoch [5/80]\n",
      "Train Loss: 68.1516 | Avg Train Acc: 0.1755\n",
      "Val Loss: 66.9488 | Avg Val Acc: 0.1728\n",
      "Task Accs (0-25): ['0.1620', '0.1717', '0.1670', '0.1720', '0.1903']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 5 | Val Acc: 0.1728)\n",
      "Best model saved (Epoch: 6 | Val Acc: 0.1856)\n",
      "Best model saved (Epoch: 7 | Val Acc: 0.1974)\n",
      "Best model saved (Epoch: 8 | Val Acc: 0.2069)\n",
      "Epoch [9/80]\n",
      "Train Loss: 61.8017 | Avg Train Acc: 0.2238\n",
      "Val Loss: 60.7158 | Avg Val Acc: 0.2165\n",
      "Task Accs (0-25): ['0.2060', '0.2040', '0.2163', '0.2107', '0.2297']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 9 | Val Acc: 0.2165)\n",
      "Best model saved (Epoch: 10 | Val Acc: 0.2227)\n",
      "Best model saved (Epoch: 11 | Val Acc: 0.2286)\n",
      "Best model saved (Epoch: 12 | Val Acc: 0.2360)\n",
      "Epoch [13/80]\n",
      "Train Loss: 58.6428 | Avg Train Acc: 0.2512\n",
      "Val Loss: 57.4809 | Avg Val Acc: 0.2402\n",
      "Task Accs (0-25): ['0.2403', '0.2283', '0.2367', '0.2490', '0.2503']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 13 | Val Acc: 0.2402)\n",
      "Best model saved (Epoch: 14 | Val Acc: 0.2460)\n",
      "Best model saved (Epoch: 15 | Val Acc: 0.2474)\n",
      "Best model saved (Epoch: 16 | Val Acc: 0.2509)\n",
      "Epoch [17/80]\n",
      "Train Loss: 56.5808 | Avg Train Acc: 0.2715\n",
      "Val Loss: 55.4972 | Avg Val Acc: 0.2571\n",
      "Task Accs (0-25): ['0.2487', '0.2457', '0.2773', '0.2597', '0.2867']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 17 | Val Acc: 0.2571)\n",
      "Best model saved (Epoch: 18 | Val Acc: 0.2592)\n",
      "Best model saved (Epoch: 19 | Val Acc: 0.2627)\n",
      "Best model saved (Epoch: 20 | Val Acc: 0.2660)\n",
      "Epoch [21/80]\n",
      "Train Loss: 55.2689 | Avg Train Acc: 0.2852\n",
      "Val Loss: 54.1259 | Avg Val Acc: 0.2681\n",
      "Task Accs (0-25): ['0.2720', '0.2507', '0.2760', '0.2793', '0.2917']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 21 | Val Acc: 0.2681)\n",
      "Best model saved (Epoch: 22 | Val Acc: 0.2716)\n",
      "Best model saved (Epoch: 23 | Val Acc: 0.2748)\n",
      "Best model saved (Epoch: 24 | Val Acc: 0.2774)\n",
      "Epoch [25/80]\n",
      "Train Loss: 54.0471 | Avg Train Acc: 0.2979\n",
      "Val Loss: 52.9646 | Avg Val Acc: 0.2801\n",
      "Task Accs (0-25): ['0.2707', '0.2713', '0.3147', '0.2833', '0.3087']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 25 | Val Acc: 0.2801)\n",
      "Best model saved (Epoch: 26 | Val Acc: 0.2851)\n",
      "Best model saved (Epoch: 27 | Val Acc: 0.2872)\n",
      "Best model saved (Epoch: 28 | Val Acc: 0.2894)\n",
      "Epoch [29/80]\n",
      "Train Loss: 53.2202 | Avg Train Acc: 0.3082\n",
      "Val Loss: 51.9446 | Avg Val Acc: 0.2915\n",
      "Task Accs (0-25): ['0.2883', '0.2643', '0.3147', '0.2980', '0.3227']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 29 | Val Acc: 0.2915)\n",
      "Best model saved (Epoch: 30 | Val Acc: 0.2940)\n",
      "Best model saved (Epoch: 31 | Val Acc: 0.2962)\n",
      "Best model saved (Epoch: 32 | Val Acc: 0.2980)\n",
      "Epoch [33/80]\n",
      "Train Loss: 52.4117 | Avg Train Acc: 0.3176\n",
      "Val Loss: 50.9805 | Avg Val Acc: 0.3027\n",
      "Task Accs (0-25): ['0.2797', '0.2983', '0.3217', '0.3037', '0.3337']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 33 | Val Acc: 0.3027)\n",
      "Best model saved (Epoch: 35 | Val Acc: 0.3063)\n",
      "Best model saved (Epoch: 36 | Val Acc: 0.3085)\n",
      "Epoch [37/80]\n",
      "Train Loss: 51.7021 | Avg Train Acc: 0.3249\n",
      "Val Loss: 50.1960 | Avg Val Acc: 0.3124\n",
      "Task Accs (0-25): ['0.2993', '0.3077', '0.3397', '0.3103', '0.3503']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 37 | Val Acc: 0.3124)\n",
      "Best model saved (Epoch: 39 | Val Acc: 0.3138)\n",
      "Best model saved (Epoch: 40 | Val Acc: 0.3149)\n",
      "Epoch [41/80]\n",
      "Train Loss: 51.0083 | Avg Train Acc: 0.3333\n",
      "Val Loss: 49.6273 | Avg Val Acc: 0.3176\n",
      "Task Accs (0-25): ['0.3180', '0.3037', '0.3383', '0.3040', '0.3497']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 41 | Val Acc: 0.3176)\n",
      "Best model saved (Epoch: 42 | Val Acc: 0.3189)\n",
      "Best model saved (Epoch: 43 | Val Acc: 0.3219)\n",
      "Best model saved (Epoch: 44 | Val Acc: 0.3233)\n",
      "Epoch [45/80]\n",
      "Train Loss: 50.5472 | Avg Train Acc: 0.3382\n",
      "Val Loss: 48.9420 | Avg Val Acc: 0.3243\n",
      "Task Accs (0-25): ['0.3313', '0.3090', '0.3473', '0.3270', '0.3543']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 45 | Val Acc: 0.3243)\n",
      "Best model saved (Epoch: 46 | Val Acc: 0.3260)\n",
      "Best model saved (Epoch: 47 | Val Acc: 0.3286)\n",
      "Best model saved (Epoch: 48 | Val Acc: 0.3298)\n",
      "Epoch [49/80]\n",
      "Train Loss: 50.0870 | Avg Train Acc: 0.3438\n",
      "Val Loss: 48.3129 | Avg Val Acc: 0.3336\n",
      "Task Accs (0-25): ['0.3270', '0.3250', '0.3477', '0.3390', '0.3577']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 49 | Val Acc: 0.3336)\n",
      "Best model saved (Epoch: 52 | Val Acc: 0.3365)\n",
      "Epoch [53/80]\n",
      "Train Loss: 49.5962 | Avg Train Acc: 0.3498\n",
      "Val Loss: 47.8662 | Avg Val Acc: 0.3389\n",
      "Task Accs (0-25): ['0.3280', '0.3257', '0.3597', '0.3383', '0.3653']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 53 | Val Acc: 0.3389)\n",
      "Best model saved (Epoch: 55 | Val Acc: 0.3393)\n",
      "Best model saved (Epoch: 56 | Val Acc: 0.3407)\n",
      "Epoch [57/80]\n",
      "Train Loss: 49.1911 | Avg Train Acc: 0.3542\n",
      "Val Loss: 47.3886 | Avg Val Acc: 0.3434\n",
      "Task Accs (0-25): ['0.3397', '0.3430', '0.3607', '0.3487', '0.3657']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 57 | Val Acc: 0.3434)\n",
      "Best model saved (Epoch: 59 | Val Acc: 0.3452)\n",
      "Best model saved (Epoch: 60 | Val Acc: 0.3458)\n",
      "Epoch [61/80]\n",
      "Train Loss: 48.8744 | Avg Train Acc: 0.3583\n",
      "Val Loss: 47.0125 | Avg Val Acc: 0.3461\n",
      "Task Accs (0-25): ['0.3423', '0.3290', '0.3630', '0.3503', '0.3723']...\n",
      "--------------------------------------------------------------------------------\n",
      "Best model saved (Epoch: 61 | Val Acc: 0.3461)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m best_val_acc = \u001b[32m0.0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# 训练一个epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     train_loss, train_accs = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# 验证一个epoch\u001b[39;00m\n\u001b[32m     23\u001b[39m     val_loss, val_accs = val_epoch(model, val_loader, criterion, DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     66\u001b[39m y_batch = y_batch.to(device)  \u001b[38;5;66;03m# (batch_size, 26)\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 前向传播：获取26个任务的输出（每个输出是(batch_size, 26)）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# 计算每个任务的损失，求和作为总损失\u001b[39;00m\n\u001b[32m     72\u001b[39m loss = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mMultiTaskMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"前向传播：输入→共享特征→26个任务输出\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m shared_features = \u001b[38;5;28mself\u001b[39m.shared_layers(x)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m outputs = [\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_features\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task_heads]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X_train_tensor, X_val_tensor = preprocess_X(X_train, X_val)\n",
    "y_train_tensor = preprocess_y(y_train)\n",
    "y_val_tensor = preprocess_y(y_val)\n",
    "\n",
    "train_dataset = MultiTaskDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = MultiTaskDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ---------------------- 初始化模型、损失函数、优化器 ----------------------\n",
    "model = MultiTaskMLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()#label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# ---------------------- 模型训练 ----------------------\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    # 训练一个epoch\n",
    "    train_loss, train_accs = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    # 验证一个epoch\n",
    "    val_loss, val_accs = val_epoch(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # 学习率衰减（根据验证损失）\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # 计算平均准确率（所有任务的均值）\n",
    "    avg_train_acc = np.mean(train_accs)\n",
    "    avg_val_acc = np.mean(val_accs)\n",
    "    \n",
    "    # 打印日志\n",
    "    if epoch%4 == 0 or epoch == EPOCHS-1:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Avg Train Acc: {avg_train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Avg Val Acc: {avg_val_acc:.4f}\")\n",
    "        print(f\"Task Accs (0-25): {[f'{acc:.4f}' for acc in val_accs[:5]]}...\")  # 打印前5个任务的准确率\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        torch.save(model.state_dict(), \"best_multi_task_mlp.pth\")\n",
    "        print(f\"Best model saved (Epoch: {epoch+1} | Val Acc: {best_val_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5ee110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation samples:\n",
      "True key: l j v f n a s h w z k b d o g c u p q x m t e i y r\n",
      "Pred key: l q x f n i s y z x w x m n d c u c j w l a e a g i\n",
      "-----------------------------------------------------------------\n",
      "True key: n s w x k g r e j l f b p z y u d c t v h o q m a i\n",
      "Pred key: o s w q z g r e j m f v y x y u d p t q h n w x e i\n",
      "-----------------------------------------------------------------\n",
      "True key: l n u z e v o p q b f d s m r j k h g y w i c t a x\n",
      "Pred key: l s u z e q o c j b f p s d n v q c g h w i p o a k\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- 模型预测 ----------------------\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load(\"best_multi_task_mlp.pth\"))\n",
    "# 对验证集预测\n",
    "y_pred = predict(model, X_val_tensor, DEVICE)\n",
    "\n",
    "# 打印预测结果示例（前3个样本）\n",
    "print(\"\\nValidation samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"True key: {' '.join( [i for i in list(y_val.iloc[i]) if i in letters] )}\")\n",
    "    print(f\"Pred key: {' '.join(y_pred[i])}\")\n",
    "    print(\"-\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712693ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
